The activation function is highly influential for the results.
I got poor results when using sigmoid which squeezes the  q_values between 0 and 1,
For tsetlin it might have to be high as well.
So regression tsetlin machine is probably necessary.
We might also have to be careful of how high the output range of the rtm should be, as it may affect training time.

Plan going forward:
1. Get average 200 points over 100 runs.
2. Experiment with ranges of q_values and see what the lower threshold for good results is.
3. Research and implement RTM


(note when experimenting with sigmoid, it needs to be multiplied with very high numbers to show signs of learning.
Although it definitely makes it quite unstable.)
lowest number so far c=50

using sigmoid function makes the results unstable, it seems to perform best at c=80, higher or lower gives poor results.

https://github.com/cair/tmu
Regression tsetlin machine.
1. Use two regression tsetlin machines, one for each action.
    - they are updated individually,
    - the one that made the action is updated I think.
    - so you get the target q_val (for the action taken) and then you update that rtm
2. Discretize the input
    -doubt I'll have to binarize the entire observation space, so generate values from DQN and then use them to define the binarizer.
3. normalize the output by multiplying with 80 or so.
4. have a lot of clauses to increase the floating point values possible(don't think so)
5.

TMU library doesnt seem to work for RTM, changing clauses affects the output range.
Solution: It requires a high T to work.

Thoughts:
Try to make the difference in q-values larger if they are too similar.
this may be done by using something similar to advantage for PPO.
or atleast an algorithm that takes into account all future rewards.


When doing negative reinforcement other tm -> y_max - target_q_val it learns, but approximates 1.0 for its target_q_val
most values by the tms is below halfway point of the tm y_max.

Does the rtm give positive reinforcemnt if the value is higher than y_max / 2 and negative if it is below?
No, it bases it on whether the predicted value is higher or lower than the one we calculated ourselves.
if the prediction is the same before training the model, then using the same input should produce the same output,
this means that adding a reward +1 will cause positive reinforcement, and the next state being "done" will very likely result in negative reinforcement

calculate target q values for both tms not just one like it is now, that wont work i atleast haveto know which
wait no probably not target q-val is the expected advantage

SOLUTION: calculate argmin as well and then update based on that as well if it is
not the solution

If the tm predicts a lower value than the target_q_val it will get positive feedback, making the clauses move closer to predicting 1 each
If the TM predicts a higher value than the target_q_val it will get negative feedback, making the clauses move closer to predicting 0 each

Action 1 was taken, meaning TM 1 had a higher q_value than TM 2

----------------------------------------------------------------
TM 1 predicts 9, target_q_val = 10 -> positive feedback (must i know which TM gave the target q value?)
what does TM2 do here?

TM 1 predicts 9, target_q_val = 1 -> negative feedback
what does TM2 do here?

How do i know if the action was correct?
I know it based on whether the next state was done(fail) or not,
so I want to give feedback to the TM that made the decision,

- if it failed (done) then the TM should be given negative feedback to lower the q_value it predicts, (this means a lower target q val)
  (and ideally the other TM should be given positive feedback to increase the q_value it predicts) (this means a high target q val for the other tm (y_max))
  (We may not know if that would not result in a fail, but maybe we should assume)

-if it succeeded then the TM should be given positive feedback to increase the q_value it predicts. (this means a higher target q val)
 (should the other TM be given negative feedback to reduce the q_value it predicts?)
 (it may not be a bad action for the tm, we also don't know if it would have resulted in a fail or not) if we are to have this it means a lower target_q_val (y_min))



